{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97073560",
   "metadata": {},
   "outputs": [],
   "source": [
    " Fraud Detection with Logistic Regression and Feature Engineering\n",
    "\n",
    "You are a data scientist at a financial institution, and your primary task is to develop a fraud detection model\n",
    "using logistic regression. The dataset you have is highly imbalanced, with only a small fraction of transactions \n",
    "being fraudulent. Your objective is to create an effective model by implementing logistic regression and employing\n",
    "various feature engineering techniques to improve the model's performance:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "a. Load the dataset, and provide an overview of the available features, including transaction\n",
    "\n",
    "details, customer information, and labels (fraudulent or non-fraudulent).\n",
    "\n",
    "b. Describe the class distribution of fraudulent and non-fraudulent transactions and discuss the imbalance issue.\n",
    "\n",
    "\n",
    "2. Initial Logistic Regression Model:\n",
    "\n",
    "a. Implement a basic logistic regression model using the raw dataset.\n",
    "\n",
    "b. Evaluate the model's performance using standard metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "3. Feature Engineering:\n",
    "\n",
    "a. Apply feature engineering techniques to enhance the predictive power of the model. These techniques may include:\n",
    "\n",
    "- Creating new features.\n",
    "\n",
    "- Scaling or normalizing features.\n",
    "\n",
    "- Handling missing values.\n",
    "\n",
    "-Encoding categorical variables.\n",
    "\n",
    "b. Explain why each feature engineering technique is relevant for fraud detection.\n",
    "\n",
    "4. Handling Imbalanced Data:\n",
    "\n",
    "a. Discuss the challenges associated with imbalanced datasets in the context of fraud detection.\n",
    "\n",
    "b. Implement strategies to address class imbalance, such as:\n",
    "\n",
    "- Oversampling the minority class.\n",
    "\n",
    "- Undersampling the majority class.\n",
    "\n",
    "Using synthetic data generation techniques (e.g., SMOTE).\n",
    "\n",
    "5. Logistic Regression with Feature-Engineered Data:\n",
    "\n",
    "a. Train a logistic regression model using the feature-engineered dataset and the methods for handling imbalanced data.\n",
    "\n",
    "b. Evaluate the model's performance using appropriate evaluation metrics\n",
    "\n",
    "6. Model Interpretation:\n",
    "\n",
    "a. Interpret the coefficients of the logistic regression model and discuss which features have the most influence on fraud detection.\n",
    "\n",
    "b. Explain how the logistic regression model can be used for decision-making in identifying potential fraud.\n",
    "\n",
    "7. Model Comparison:\n",
    "\n",
    "a. Compare the performance of the initial logistic regression model with the feature-engineered and balanced data model.\n",
    "\n",
    "b. Discuss the advantages and limitations of each approach\n",
    "8. Presentation and Recommendations:\n",
    "\n",
    "a. Prepare a presentation or report summarizing your analysis, results, and recommendations for the financial institution. Highlight the importance of feature engineering and handling imbalanced data in building an effective fraud detection system.\n",
    "\n",
    "In this case study, you are required to showcase your ability to preprocess data, implement logistic regression, apply feature engineering techniques, and address class imbalance to improve the model's performance. Your analysis should also demonstrate \n",
    "your understanding of the nuances of fraud detection in a financial context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e3bc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>2.207101</td>\n",
       "      <td>0.112651</td>\n",
       "      <td>1.626798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>19.872726</td>\n",
       "      <td>2.683904</td>\n",
       "      <td>2.778303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>2.914857</td>\n",
       "      <td>1.472687</td>\n",
       "      <td>0.218075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>4.258729</td>\n",
       "      <td>0.242023</td>\n",
       "      <td>0.475822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>58.108125</td>\n",
       "      <td>0.318110</td>\n",
       "      <td>0.386920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "0                57.877857                        0.311140   \n",
       "1                10.829943                        0.175592   \n",
       "2                 5.091079                        0.805153   \n",
       "3                 2.247564                        5.600044   \n",
       "4                44.190936                        0.566486   \n",
       "...                    ...                             ...   \n",
       "999995            2.207101                        0.112651   \n",
       "999996           19.872726                        2.683904   \n",
       "999997            2.914857                        1.472687   \n",
       "999998            4.258729                        0.242023   \n",
       "999999           58.108125                        0.318110   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                             1.945940              1.0        1.0   \n",
       "1                             1.294219              1.0        0.0   \n",
       "2                             0.427715              1.0        0.0   \n",
       "3                             0.362663              1.0        1.0   \n",
       "4                             2.222767              1.0        1.0   \n",
       "...                                ...              ...        ...   \n",
       "999995                        1.626798              1.0        1.0   \n",
       "999996                        2.778303              1.0        1.0   \n",
       "999997                        0.218075              1.0        1.0   \n",
       "999998                        0.475822              1.0        0.0   \n",
       "999999                        0.386920              1.0        1.0   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "0                   0.0           0.0    0.0  \n",
       "1                   0.0           0.0    0.0  \n",
       "2                   0.0           1.0    0.0  \n",
       "3                   0.0           1.0    0.0  \n",
       "4                   0.0           1.0    0.0  \n",
       "...                 ...           ...    ...  \n",
       "999995              0.0           0.0    0.0  \n",
       "999996              0.0           0.0    0.0  \n",
       "999997              0.0           1.0    0.0  \n",
       "999998              0.0           1.0    0.0  \n",
       "999999              0.0           1.0    0.0  \n",
       "\n",
       "[1000000 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Load the dataset (replace 'your_dataset.csv' with the actual dataset file path)\n",
    "df = pd.read_csv('card_transdata.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef9906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud\n",
      "0.0    912597\n",
      "1.0     87403\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = df['fraud'].value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01efa770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.drop(columns=['fraud'])\n",
    "y = df['fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1455082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.89\n",
      "Recall: 0.60\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf69ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1786a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['distance_from_home', 'distance_from_last_transaction',\n",
      "       'ratio_to_median_purchase_price', 'repeat_retailer', 'used_chip',\n",
      "       'used_pin_number', 'online_order', 'fraud', 'transaction_frequency',\n",
      "       'normalized_amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Q3.# Example: Create a new feature for transaction frequency\n",
    "df['transaction_frequency'] = df.groupby('distance_from_home')['distance_from_last_transaction'].transform('count')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['normalized_amount'] = scaler.fit_transform(df['distance_from_home'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Check the column names in your DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "# Replace 'missing_column' with the actual column name\n",
    "column_name = 'repeat_retailer'  # Replace with the correct column name\n",
    "df[column_name].fillna(df[column_name].mean(), inplace=True)\n",
    "\n",
    "\n",
    "# Example: One-hot encoding a categorical variable\n",
    "df = pd.get_dummies(df, columns=['used_chip'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a800a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac47617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f6609e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE  \u001b[38;5;66;03m# Import SMOTE for oversampling\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming you have already performed the feature engineering steps mentioned earlier.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset with the feature-engineered data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_engineered_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "#Q5.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE for oversampling\n",
    "\n",
    "# Assuming you have already performed the feature engineering steps mentioned earlier.\n",
    "\n",
    "# Load the dataset with the feature-engineered data\n",
    "df = pd.read_csv('feature_engineered_data.csv')\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.drop(columns=['fraud'])\n",
    "y = df['fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30d63802",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3050803057.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install --upgrade imbalanced-learn\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a2e84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Feature  Coefficient\n",
      "6                    online_order     6.636062\n",
      "2  ratio_to_median_purchase_price     0.858748\n",
      "1  distance_from_last_transaction     0.025416\n",
      "0              distance_from_home     0.015106\n",
      "3                 repeat_retailer    -0.613280\n",
      "4                       used_chip    -1.039381\n",
      "5                 used_pin_number   -13.212769\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "coefficients = model.coef_\n",
    "feature_names = X.columns\n",
    "\n",
    "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients[0]})\n",
    "coef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ef06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
