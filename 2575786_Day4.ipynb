{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64dc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad241528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Initial Linear Regression Model:\n",
    "\n",
    "# a. Describe the dataset and the variables you're using for predicting employee performance.\n",
    "data = pd.read_csv('company_employee_details.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e71a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Implement a simple linear regression model:\n",
    "X = data[['salary', 'annual_bonus', 'contractor']]\n",
    "y = data['age_when_joined']\n",
    "\n",
    "# Add a constant for the intercept in the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the initial linear regression model\n",
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Methods for diagnosing heteroscedasticity:\n",
    "# Residuals vs. Fitted plot and Breusch-Pagan test\n",
    "residuals = model.resid\n",
    "predicted = model.fittedvalues\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted')\n",
    "plt.show()\n",
    "\n",
    "# Apply Breusch-Pagan test\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "bp_test = het_breuschpagan(residuals, X)\n",
    "print(\"Breusch-Pagan test p-value:\", bp_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33241a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Ways to address heteroscedasticity:\n",
    "# Transform variables (e.g., log transformation) or use weighted least squares (WLS) regression.\n",
    "\n",
    "# Log-transform the response variable\n",
    "y = np.log(y)\n",
    "\n",
    "# Fit a WLS model\n",
    "wls_model = sm.WLS(y, X, weights=1/predicted).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Ways to address heteroscedasticity:\n",
    "# Transform variables (e.g., log transformation) or use weighted least squares (WLS) regression.\n",
    "\n",
    "# Log-transform the response variable\n",
    "y = np.log(y)\n",
    "\n",
    "# Fit a WLS model\n",
    "wls_model = sm.WLS(y, X, weights=1/predicted).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad303c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define a list of K values to test\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "\n",
    "# Dictionary to store results\n",
    "k_results = {}\n",
    "\n",
    "# Iterate through different K values\n",
    "for k in k_values:\n",
    "    # Create the KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and store it in the results dictionary\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    k_results[k] = accuracy\n",
    "\n",
    "# Find the K with the highest accuracy\n",
    "optimal_k = max(k_results, key=k_results.get)\n",
    "\n",
    "print(\"Accuracy results for different K values:\")\n",
    "for k, accuracy in k_results.items():\n",
    "    print(f\"K = {k}: Accuracy = {accuracy}\")\n",
    "\n",
    "print(f\"The optimal K for better performance is {optimal_k} with an accuracy of {k_results[optimal_k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109942de",
   "metadata": {},
   "source": [
    "# Q1:- Medical Diagnosis with Naive Bayes\n",
    "\n",
    "You work for a medical research institute, and your task is to develop a diagnostic system using the Naive \n",
    "Bayes algorithm. You have a dataset with various medical test results, patient information, and corresponding \n",
    "diagnoses (e.g., presence or absence of a medical condition). Your goal is to create a classification model to\n",
    "aid in the medical diagnosis process. Answer the following questions based on this case study:\n",
    "\n",
    "1. Data Exploration:\n",
    "\n",
    "a. Load and explore the medical dataset using Python libraries like pandas. Describe the features, labels, \n",
    "and the distribution of diagnoses.\n",
    "\n",
    "2. Data Preprocessing:\n",
    "\n",
    "a. Explain the necessary data preprocessing steps for preparing the medical data. This may include handling missing values, normalizing or scaling features, and encoding categorical variables.\n",
    "\n",
    "b. Calculate the prior probabilities P(Condition) and P(No Condition) based on the class distribution.\n",
    "\n",
    "3. Feature Engineering:\n",
    "\n",
    "a. Describe how to convert the medical test results and patient information into suitable features for the Naive Bayes model.\n",
    "\n",
    "b. Discuss the importance of feature selection or dimensionality reduction in medical diagnosis.\n",
    "\n",
    "4. Implementing Na√Øve Bayes:\n",
    "\n",
    "a. Choose the appropriate Naive Bayes variant (e.g., Gaussian, Multinomial, or Bernoulli Naive Bayes) for the medical diagnosis task and implement the classifier using Python libraries like scikit-learn.\n",
    "\n",
    "b. Split the dataset into training and testing sets.\n",
    "\n",
    "5. Model Training:\n",
    "\n",
    "a. Train the Naive Bayes model using the feature-engineered dataset. Explain the probability estimation process in Naive Bayes for medical diagnosis.\n",
    "6. Model Evaluation:\n",
    "\n",
    "a. Assess the performance of the medical diagnosis model using relevant evaluation metrics, such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "b. Interpret the results and discuss the model's ability to accurately classify medical conditions.\n",
    "\n",
    "7. Laplace Smoothing:\n",
    "\n",
    "a. Explain the concept of Laplace (add-one) smoothing and discuss its potential application in the context of medical diagnosis.\n",
    "\n",
    "b. Discuss the impact of Laplace smoothing on model performance.\n",
    "\n",
    "8. Real-World Application:\n",
    "\n",
    "a. Describe the importance of accurate medical diagnosis in healthcare and research.\n",
    "\n",
    "b. Discuss the practical implications of implementing a diagnostic system based on Naive Bayes.\n",
    "\n",
    "9. Model Limitations:\n",
    "\n",
    "a. Identify potential limitations of the Naive Bayes approach to medical diagnosis and discuss scenarios in which it may not perform well.\n",
    "\n",
    "10. Presentation and Recommendations:\n",
    "\n",
    "a. Prepare a presentation or report summarizing your analysis, results, and recommendations for the medical research institute. Highlight the significance of accurate medical diagnosis and the role of Naive Bayes in healthcare.\n",
    "\n",
    "In this case study, you can demonstrate your ability to apply the Naive Bayes algorithm to non-text data, understand the nuances of feature engineering for different types of data, and assess the model's performance in a critical domain such as medical diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00999ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Exploration:\n",
    "\n",
    "# a. Load and explore the medical dataset using Python libraries like pandas. Describe the features, labels,\n",
    "# and the distribution of diagnoses.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46211752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics for numerical features\n",
    "numerical_stats = df.describe()\n",
    "print(numerical_stats)\n",
    "\n",
    "print()\n",
    "\n",
    "# Explore the distribution of diagnoses\n",
    "diagnosis_distribution = df['Outcome'].value_counts()\n",
    "print(diagnosis_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Calculate the prior probabilities P(Condition) and P(No Condition) based on the class distribution.\n",
    "\n",
    "condition_prob = df['Outcome'].value_counts(normalize=True)\n",
    "print(condition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81706ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implementing Na√Øve Bayes:\n",
    "\n",
    "# a. Choose the appropriate Naive Bayes variant (e.g., Gaussian, Multinomial, or Bernoulli Naive Bayes) for the medical\n",
    "# diagnosis task and implement the classifier using Python libraries like scikit-learn.\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df31620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Split the dataset into training and testing sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define labels\n",
    "labels = df['Outcome']\n",
    "\n",
    "# Split the dataset into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Outcome', axis=1), labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Training:\n",
    "\n",
    "# a. Train the Naive Bayes model using the feature-engineered dataset. Explain the probability estimation process in Naive\n",
    "# Bayes for medical diagnosis.\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1651d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Evaluation:\n",
    "\n",
    "# a. Assess the performance of the medical diagnosis model using relevant evaluation metrics, such as accuracy, precision,\n",
    "# recall, and F1-score.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d289d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Interpret the results and discuss the model's ability to accurately classify medical conditions.\n",
    "\n",
    "Accuracy: Accuracy provides an overall measure of how well the model is performing. A high accuracy indicates that the\n",
    "model is making correct predictions for a large portion of the dataset. However, in imbalanced datasets (where one\n",
    "class is significantly larger than the other), accuracy alone may not be a reliable indicator.\n",
    "\n",
    "Precision: Precision is crucial in medical diagnosis because it measures how well the model identifies true positives\n",
    "while minimizing false positives. High precision means that the model is accurate when it predicts a positive case.\n",
    "\n",
    "Recall: Recall is equally important, especially in cases where its crucial not to miss positive cases. High recall\n",
    "indicates that the model captures a large portion of the actual positive cases.\n",
    "\n",
    "F1-Score: The F1-score balances precision and recall. It is useful when you want a single metric that considers both false\n",
    "positives and false negatives. A higher F1-score indicates a better trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3c9c4",
   "metadata": {},
   "source": [
    "# Q2:- \n",
    "    \n",
    "Customer Segmentation with K-Nearest Neighbors (KNN)\n",
    "\n",
    "ED Info\n",
    "\n",
    "You work for a retail company, and your task is to segment customers based on their purchase behavior ing the K-Nearest Neighbors ( algorithm. The dataset.contains information about customers, such as purchase Nistory, age, and income. Your goal is to create customer segments for targeted marketing Answer the following questions based on this case study.\n",
    "\n",
    "1. Data Exploration:\n",
    "\n",
    "a. Load the customer dataset using Python libraries like pandas and explore ts structure. Describe the features, target variable, and data distribution\n",
    "\n",
    "Explain the importance of customer segmentation in the retail industry\n",
    "\n",
    "2. Data Preprocessing:\n",
    "\n",
    "Prepare the custome data for analysis. Discuss the steps involved in data preprocessing, such as scaling handling missing value and encoding categorical variables\n",
    "\n",
    "3. Implementing KNN\n",
    "\n",
    "a. Implement the K-Nearest Neighbors algorithm using Python ibraries like scikit learn to segment customers based on their features.\n",
    "\n",
    "b. Choose an appropriate number of neighbors 60 for the algorithm and explain your choice.\n",
    "\n",
    "4 Model Training:\n",
    "\n",
    "Train the 17 model using the preprocessed customer dataset\n",
    "\n",
    "b. Dacias the distance metric used for finding the nearest neighbors, and its significance in customer segmentation\n",
    "\n",
    "5. Customer Segmentation:\n",
    "\n",
    "a. Segment the customers based on their purchase behavior,¬†age,¬†and¬†inco&Vouolat the customer segments to gain insights into the distribution and characteristics of wach segment.\n",
    "\n",
    "6. Hyperparameter Tuning\n",
    "\n",
    "Explain the role of the hyperparameter (0) in the algorithm and suggest strategies for selecting the optimal value of K\n",
    "\n",
    "Conduct hyperparameter tuning for the 10 model and discuss the impact of different va of K on segmentation results.\n",
    "\n",
    "7. Model Evaluation:\n",
    "\n",
    "x. Evaluate the model's performance in customer segmentation Discuss the criteria and metrics used for evaluating unsupervised learning models.\n",
    "\n",
    "Interpret the results and provide insights on how the customer segments can be leveraged for marketing strategies\n",
    "\n",
    "5. Real-World Application\n",
    "\n",
    "Describe the practical applications of customer segmentation in the retail industry\n",
    "\n",
    "b. Discuss how customer segmentation can lead to improved customer engagement and increased sales\n",
    "\n",
    "9. Model Limitations\n",
    "\n",
    "Identify potential stations of the KNN algorithm in customer segmentation and discuss\n",
    "\n",
    "scenarios in which it may not perform wel\n",
    "\n",
    "10. Presentation and Recommendations:\n",
    "\n",
    "a.Prepare a presentation or report summurting your analysis, results, and recommendations for the retal company. Highlight the significance of customer segmentation and the role of the data-driven marketing\n",
    "\n",
    "in this case study, you are requered is demonstrate your ability to use the the algorthen for customer segmentation, understand the importance of hyperparameter tuning, and communicate the practical applications of customer segmentation in the¬†retall¬†sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import pandas as pd\n",
    "\n",
    "# Load the customer dataset (replace 'filename.csv' with your actual dataset file)\n",
    "customer_data = pd.read_csv('shopping_trends_updated.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to get an overview\n",
    "customer_data\n",
    "\n",
    "# Describe the features (columns) and data distribution\n",
    "print(customer_data.info())\n",
    "print(customer_data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the customer dataset (replace 'customer_data.csv' with your actual file path or URL)\n",
    "data = pd.read_csv('shopping_trends_updated.csv')\n",
    "\n",
    "# Define your numerical and categorical features\n",
    "numerical_features = ['Previous Purchases']  # Replace with your actual numerical feature column names\n",
    "categorical_features = ['Review Rating', 'Item Purchased', 'Shipping Type']  # Replace with your actual categorical feature column names\n",
    "\n",
    "# Check if the specified categorical features exist in the dataset\n",
    "for col in categorical_features:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Column '{col}' specified in categorical_features does not exist in the dataset.\")\n",
    "\n",
    "# Step 1: Handle Missing Values\n",
    "# For numerical features, fill missing values with the mean\n",
    "imputer_numeric = SimpleImputer(strategy='mean')\n",
    "data[numerical_features] = imputer_numeric.fit_transform(data[numerical_features])\n",
    "\n",
    "# For categorical features, fill missing values with the most frequent category\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_features] = imputer_categorical.fit_transform(data[categorical_features])\n",
    "\n",
    "# Step 2: Encode Categorical Variables\n",
    "# We'll use one-hot encoding for categorical features to create binary columns for each category.\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # Set 'drop' to 'first' to avoid multicollinearity\n",
    "encoded_features = encoder.fit_transform(data[categorical_features])\n",
    "encoded_feature_names = encoder.get_feature_names_out(input_features=categorical_features)\n",
    "encoded_data = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "# Step 3: Combine Encoded Categorical Features with Numerical Features\n",
    "X = pd.concat([data[numerical_features], encoded_data], axis=1)\n",
    "\n",
    "# Step 4: Scale Numerical Features\n",
    "# It's important to scale numerical features when using K-Nearest Neighbors (KNN) as it's distance-based.\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "# Now, your data is preprocessed and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22336a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.a\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assuming 'segments' is the target variable\n",
    "X = customer_data[['Purchase Amount (USD)', 'Age', 'Previous Purchases']]\n",
    "y = customer_data['Subscription Status']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define a range of K values to test\n",
    "k_values = list(range(1, 11))\n",
    "\n",
    "# Evaluate each K value using cross-validation\n",
    "for k in k_values:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn_model, X_scaled, y, cv=5)  # 5-fold cross-validation\n",
    "    average_accuracy = scores.mean()\n",
    "    print(f'K = {k}, Average Accuracy: {average_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.b\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a range of K values to test\n",
    "k_values = list(range(1, 11))\n",
    "\n",
    "# Evaluate each K value using cross-validation\n",
    "for k in k_values:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn_model, X_scaled, y, cv=5)  # 5-fold cross-validation\n",
    "    average_accuracy = scores.mean()\n",
    "    print(f'K = {k}, Average Accuracy: {average_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_scaled is the preprocessed feature matrix, and y is the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming k_value is the chosen number of neighbors\n",
    "k_value = 5\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k_value)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy:¬†{accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cb21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load your data (replace 'your_data.csv' with your actual data)\n",
    "data = pd.read_csv('shopping_trends_updated.csv')\n",
    "\n",
    "# Select the features for segmentation (e.g., 'Review Rating', 'Age', 'Payment Method')\n",
    "selected_features = data[['Review Rating', 'Age', 'Payment Method']]\n",
    "\n",
    "# Define which columns are categorical and which are numerical\n",
    "categorical_features = [\"Payment Method\"]\n",
    "numerical_features = [\"Review Rating\", \"Age\"]\n",
    "\n",
    "# Create transformers for preprocessing\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(sparse=False))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine the transformers using a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessing\n",
    "preprocessed_features = preprocessor.fit_transform(selected_features)\n",
    "\n",
    "# Determine the optimal number of clusters (K) using the Elbow Method\n",
    "wcss = []  # Within-Cluster Sum of Squares\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(preprocessed_features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Based on the Elbow Method, choose an appropriate K (number of clusters)\n",
    "k = 3  # Adjust this value based on the Elbow Method plot\n",
    "\n",
    "# Apply K-Means clustering with the chosen K\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "data['Cluster'] = kmeans.fit_predict(preprocessed_features)\n",
    "\n",
    "# Visualize the customer segments\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster in range(k):\n",
    "    plt.scatter(data[data['Cluster'] == cluster]['Age'], data[data['Cluster'] == cluster]['Review Rating'], label=f'Cluster {cluster}')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 0], s=300, c='red', label='Centroids')\n",
    "plt.title('Customer Segmentation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Review Rating')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54790339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define a range of K values to test\n",
    "param_grid = {'n_neighbors': list(range(1, 21))}\n",
    "\n",
    "# Create KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Get the best hyperparameter values\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(f'Best K: {best_k}, Best Accuracy: {best_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35119d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load your preprocessed data\n",
    "data = pd.read_csv(\"shopping_trends_updated.csv\")\n",
    "\n",
    "# Define your features for clustering (e.g., using numerical columns)\n",
    "X = data[['Review Rating', 'Previous Purchases']]\n",
    "\n",
    "# Choose the number of clusters (you can experiment with different values)\n",
    "n_clusters = 5\n",
    "\n",
    "# Fit K-Means clustering model\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate silhouette score to evaluate the model\n",
    "silhouette_avg = silhouette_score(X, data['Cluster'])\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# Analyze the clusters\n",
    "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=['Review Rating', 'Previous Purchases'])\n",
    "cluster_centers['Cluster'] = range(n_clusters)\n",
    "print(cluster_centers)\n",
    "\n",
    "# Interpretation and marketing insights can be provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e688a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming customer_data is the preprocessed DataFrame with features\n",
    "X = customer_data[['Purchase Amount (USD)', 'Age', 'Previous Purchases']]\n",
    "\n",
    "# Choose the number of clusters (segments)\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "customer_data['Subscription Status'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Assume you have a product catalog DataFrame\n",
    "product_catalog = pd.DataFrame({\n",
    "    'product_id': range(1, 11),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 11)]\n",
    "})\n",
    "\n",
    "# Define a function to recommend products for a given customer segment\n",
    "def recommend_products(segment):\n",
    "    segment_products = product_catalog.sample(3)  # Recommend 3 random products\n",
    "    return segment_products\n",
    "\n",
    "# Example: Recommend products for customers in segment 0\n",
    "segment_0_recommendations = recommend_products(0)\n",
    "print(\"Product Recommendations for Segment 0:\")\n",
    "print(segment_0_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec464ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a synthetic dataset with outliers\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X[-1] = [4, 2]  # Adding an outlier\n",
    "\n",
    "# Train KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X, y)\n",
    "\n",
    "# Plot decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = knn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the dataset and decision boundary\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=100)\n",
    "plt.title(\"KNN Decision Boundary with Outlier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
